{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Recommendation System - Random Baseline\n",
    "\n",
    "Test if geographic filtering alone (without learning) provides good recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.metrics import evaluate_recommendations\n",
    "from utils.temporal_split import temporal_split_per_user, print_split_stats\n",
    "from utils.geo_filter import haversine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw train data: 15398 interactions\n",
      "Events: 3137972\n"
     ]
    }
   ],
   "source": [
    "raw_dir = Path(\"../data/raw\")\n",
    "\n",
    "train_raw = pd.read_csv(raw_dir / \"train.csv\")\n",
    "events_raw = pd.read_csv(raw_dir / \"events.csv\")\n",
    "\n",
    "print(f\"Raw train data: {len(train_raw)} interactions\")\n",
    "print(f\"Events: {len(events_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL SPLIT STATISTICS\n",
      "============================================================\n",
      "\n",
      "TRAIN SET:\n",
      "  Total interactions: 7393\n",
      "  Unique users: 2034\n",
      "  Unique events: 4733\n",
      "  Interested=1: 1337\n",
      "\n",
      "VALIDATION SET:\n",
      "  Total interactions: 8005\n",
      "  Unique users: 2034\n",
      "  Unique events: 5127\n",
      "  Interested=1: 2794\n",
      "\n",
      "OVERLAP:\n",
      "  Users in both: 2034\n",
      "  Events in both: 1014\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = temporal_split_per_user(train_raw, train_ratio=0.5, min_interactions=3)\n",
    "\n",
    "print_split_stats(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processed Events (for geo coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached processed events...\n",
      "Processed events shape: (3137972, 113)\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import EventFeatureExtractor\n",
    "\n",
    "processed_events_path = Path(\"../data/processed/events_processed.csv\")\n",
    "\n",
    "if processed_events_path.exists():\n",
    "    print(\"Loading cached processed events...\")\n",
    "    events = pd.read_csv(processed_events_path)\n",
    "else:\n",
    "    print(\"Processing events...\")\n",
    "    extractor = EventFeatureExtractor(n_clusters=30)\n",
    "    events = extractor.fit_transform(events_raw)\n",
    "    events.to_csv(processed_events_path, index=False)\n",
    "\n",
    "print(f\"Processed events shape: {events.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Baseline with Geographic Filtering\n",
    "\n",
    "This baseline:\n",
    "1. Finds user's median location from past events\n",
    "2. Filters to top-K nearest events (geo_top_k)\n",
    "3. **Randomly samples 200 events** from this pool\n",
    "4. No learning - just geographic proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline function ready\n"
     ]
    }
   ],
   "source": [
    "def get_user_median_location(user_id, train_df, events):\n",
    "    \"\"\"Get user's median location from their past interactions\"\"\"\n",
    "    user_events = train_df[train_df[\"user\"] == user_id][\"event\"].tolist()\n",
    "    \n",
    "    if not user_events:\n",
    "        return None, None\n",
    "    \n",
    "    # Use merge instead of isin for better performance\n",
    "    event_locs = events[events[\"event_id\"].isin(user_events)][[\"lat\", \"lng\"]].dropna()\n",
    "    \n",
    "    if len(event_locs) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    median_lat = event_locs[\"lat\"].median()\n",
    "    median_lng = event_locs[\"lng\"].median()\n",
    "    \n",
    "    return median_lat, median_lng\n",
    "\n",
    "\n",
    "def random_recommend_with_geo(user_id, train_df, events, geo_top_k=3000, n=200, exclude_seen=True):\n",
    "    \"\"\"Random recommendation with geographic filtering\"\"\"\n",
    "    \n",
    "    # Get user location\n",
    "    user_lat, user_lng = get_user_median_location(user_id, train_df, events)\n",
    "    \n",
    "    if user_lat is None or pd.isna(user_lat) or pd.isna(user_lng):\n",
    "        # Fallback: random sample from all events\n",
    "        candidates = events[\"event_id\"].sample(min(geo_top_k, len(events))).tolist()\n",
    "    else:\n",
    "        # Pre-filter events with valid coordinates\n",
    "        valid_events = events.dropna(subset=[\"lat\", \"lng\"]).copy()\n",
    "        \n",
    "        # Vectorized distance calculation\n",
    "        valid_events[\"distance\"] = haversine_distance(\n",
    "            user_lat, user_lng, \n",
    "            valid_events[\"lat\"].values, \n",
    "            valid_events[\"lng\"].values\n",
    "        )\n",
    "        \n",
    "        # Get top-K nearest events\n",
    "        nearest_events = valid_events.nsmallest(geo_top_k, \"distance\")\n",
    "        candidates = nearest_events[\"event_id\"].tolist()\n",
    "    \n",
    "    # Exclude seen events\n",
    "    if exclude_seen:\n",
    "        seen_events = set(train_df[train_df[\"user\"] == user_id][\"event\"])\n",
    "        candidates = [e for e in candidates if e not in seen_events]\n",
    "    \n",
    "    # Randomly sample n events\n",
    "    if len(candidates) <= n:\n",
    "        return candidates\n",
    "    else:\n",
    "        return list(np.random.choice(candidates, size=n, replace=False))\n",
    "\n",
    "\n",
    "print(\"Random baseline function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with labels in validation: 1501\n",
      "Evaluating on 100 users...\n",
      "\n",
      "==================================================\n",
      "RANDOM BASELINE (geo_top_k=3000) @ K=200\n",
      "==================================================\n",
      "Recall@K            : 0.03833\n",
      "Hit_Rate@K          : 0.06000\n",
      "Contamination@K     : 0.00000\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "K = 200\n",
    "N_TEST_USERS = 100\n",
    "GEO_TOP_K = 3000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "val_with_labels = val_df[(val_df[\"interested\"] == 1) | (val_df[\"not_interested\"] == 1)]\n",
    "users_with_labels = val_with_labels[\"user\"].unique()\n",
    "\n",
    "print(f\"Users with labels in validation: {len(users_with_labels)}\")\n",
    "\n",
    "if N_TEST_USERS:\n",
    "    test_users = users_with_labels[:N_TEST_USERS]\n",
    "else:\n",
    "    test_users = users_with_labels\n",
    "\n",
    "print(f\"Evaluating on {len(test_users)} users...\")\n",
    "\n",
    "random_predictions = {}\n",
    "actuals = {}\n",
    "not_interested = {}\n",
    "\n",
    "for user in test_users:\n",
    "    random_predictions[user] = random_recommend_with_geo(\n",
    "        user, train_df, events, geo_top_k=GEO_TOP_K, n=K, exclude_seen=True\n",
    "    )\n",
    "    actuals[user] = val_df[(val_df[\"user\"] == user) & (val_df[\"interested\"] == 1)][\"event\"].tolist()\n",
    "    not_interested[user] = val_df[(val_df[\"user\"] == user) & (val_df[\"not_interested\"] == 1)][\"event\"].tolist()\n",
    "\n",
    "metrics = evaluate_recommendations(actuals, random_predictions, not_interested, k=K)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RANDOM BASELINE (geo_top_k={GEO_TOP_K}) @ K={K}\")\n",
    "print(f\"{'='*50}\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.5f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Different geo_top_k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 500\n",
      "============================================================\n",
      "Recall@200: 0.02760\n",
      "Hit_Rate@200: 0.07000\n",
      "Time: 103.1s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 1000\n",
      "============================================================\n",
      "Recall@200: 0.02176\n",
      "Hit_Rate@200: 0.06000\n",
      "Time: 100.6s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 2000\n",
      "============================================================\n",
      "Recall@200: 0.05310\n",
      "Hit_Rate@200: 0.10000\n",
      "Time: 110.4s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 3000\n",
      "============================================================\n",
      "Recall@200: 0.03833\n",
      "Hit_Rate@200: 0.06000\n",
      "Time: 127.2s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 5000\n",
      "============================================================\n",
      "Recall@200: 0.00667\n",
      "Hit_Rate@200: 0.02000\n",
      "Time: 133.1s\n",
      "\n",
      "============================================================\n",
      "SUMMARY - Random Baseline with Different geo_top_k\n",
      "============================================================\n",
      "geo_top_k       Recall@K        Hit_Rate@K     \n",
      "---------------------------------------------\n",
      "500             0.02760         0.07000        \n",
      "1000            0.02176         0.06000        \n",
      "2000            0.05310         0.10000        \n",
      "3000            0.03833         0.06000        \n",
      "5000            0.00667         0.02000        \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "geo_values = [500, 1000, 2000, 3000, 5000]\n",
    "results = []\n",
    "\n",
    "for geo_k in geo_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Random with geo_top_k = {geo_k}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions = {}\n",
    "    for user in test_users:\n",
    "        predictions[user] = random_recommend_with_geo(\n",
    "            user, train_df, events, geo_top_k=geo_k, n=K, exclude_seen=True\n",
    "        )\n",
    "    \n",
    "    metrics = evaluate_recommendations(actuals, predictions, not_interested, k=K)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        \"geo_top_k\": geo_k,\n",
    "        \"recall\": metrics[\"Recall@K\"],\n",
    "        \"hit_rate\": metrics[\"Hit_Rate@K\"],\n",
    "        \"contamination\": metrics[\"Contamination@K\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Recall@{K}: {metrics['Recall@K']:.5f}\")\n",
    "    print(f\"Hit_Rate@{K}: {metrics['Hit_Rate@K']:.5f}\")\n",
    "    print(f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY - Random Baseline with Different geo_top_k\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'geo_top_k':<15} {'Recall@K':<15} {'Hit_Rate@K':<15}\")\n",
    "print(f\"{'-'*45}\")\n",
    "for r in results:\n",
    "    print(f\"{r['geo_top_k']:<15} {r['recall']:<15.5f} {r['hit_rate']:<15.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Real Models\n",
    "\n",
    "Compare Random baseline with the actual models to see the improvement from learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON: Random Baseline vs Real Models\n",
      "======================================================================\n",
      "Model                          Recall@200           Hit_Rate@200        \n",
      "----------------------------------------------------------------------\n",
      "Random (geo_top_k=3000)        0.03833              0.06000             \n",
      "Content-Based                  0.10376              0.16000             \n",
      "Collaborative                  0.22255              0.29000             \n",
      "Social                         0.46997              0.59184             \n",
      "\n",
      "======================================================================\n",
      "ANALYSIS:\n",
      "======================================================================\n",
      "Content-Based improvement over Random: 170.7%\n",
      "Collaborative improvement over Random: 480.6%\n",
      "Social improvement over Random: 1126.0%\n",
      "\n",
      "======================================================================\n",
      "✓ Content-Based significantly improves over random baseline\n",
      "✓ Collaborative significantly improves over random baseline\n",
      "✓ Social provides STRONG improvement (>3x random)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Results from other notebooks (geo_top_k=3000)\n",
    "comparison = [\n",
    "    {\"model\": \"Random (geo_top_k=3000)\", \"recall\": results[3][\"recall\"], \"hit_rate\": results[3][\"hit_rate\"]},\n",
    "    {\"model\": \"Content-Based\", \"recall\": 0.10376, \"hit_rate\": 0.16000},\n",
    "    {\"model\": \"Collaborative\", \"recall\": 0.22255, \"hit_rate\": 0.29000},\n",
    "    {\"model\": \"Social\", \"recall\": 0.46997, \"hit_rate\": 0.59184},\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON: Random Baseline vs Real Models\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Model':<30} {'Recall@200':<20} {'Hit_Rate@200':<20}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for item in comparison:\n",
    "    print(f\"{item['model']:<30} {item['recall']:<20.5f} {item['hit_rate']:<20.5f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANALYSIS:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "random_recall = comparison[0][\"recall\"]\n",
    "cb_recall = comparison[1][\"recall\"]\n",
    "cf_recall = comparison[2][\"recall\"]\n",
    "social_recall = comparison[3][\"recall\"]\n",
    "\n",
    "print(f\"Content-Based improvement over Random: {((cb_recall - random_recall) / random_recall * 100):.1f}%\")\n",
    "print(f\"Collaborative improvement over Random: {((cf_recall - random_recall) / random_recall * 100):.1f}%\")\n",
    "print(f\"Social improvement over Random: {((social_recall - random_recall) / random_recall * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if cb_recall <= random_recall:\n",
    "    print(\"⚠️  WARNING: Content-Based performs WORSE than random!\")\n",
    "    print(\"   This suggests the model is not learning useful patterns.\")\n",
    "elif cb_recall < random_recall * 1.5:\n",
    "    print(\"⚠️  Content-Based barely beats random (< 50% improvement)\")\n",
    "    print(\"   Geographic filtering is doing most of the work.\")\n",
    "else:\n",
    "    print(\"✓ Content-Based significantly improves over random baseline\")\n",
    "\n",
    "if cf_recall < random_recall * 2:\n",
    "    print(\"⚠️  Collaborative improvement is moderate\")\n",
    "else:\n",
    "    print(\"✓ Collaborative significantly improves over random baseline\")\n",
    "\n",
    "if social_recall > random_recall * 3:\n",
    "    print(\"✓ Social provides STRONG improvement (>3x random)\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event-recsys-mvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
