{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Recommendation System - Random Baseline\n",
    "\n",
    "Test if geographic filtering alone (without learning) provides good recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from utils.metrics import evaluate_recommendations\n",
    "from utils.temporal_split import temporal_split_per_user, print_split_stats\n",
    "from utils.geo_filter import haversine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw train data: 15398 interactions\n",
      "Events: 3137972\n"
     ]
    }
   ],
   "source": [
    "raw_dir = Path(\"../data/raw\")\n",
    "\n",
    "train_raw = pd.read_csv(raw_dir / \"train.csv\")\n",
    "events_raw = pd.read_csv(raw_dir / \"events.csv\")\n",
    "\n",
    "print(f\"Raw train data: {len(train_raw)} interactions\")\n",
    "print(f\"Events: {len(events_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL SPLIT STATISTICS\n",
      "============================================================\n",
      "\n",
      "TRAIN SET:\n",
      "  Total interactions: 7393\n",
      "  Unique users: 2034\n",
      "  Unique events: 4733\n",
      "  Interested=1: 1337\n",
      "\n",
      "VALIDATION SET:\n",
      "  Total interactions: 8005\n",
      "  Unique users: 2034\n",
      "  Unique events: 5127\n",
      "  Interested=1: 2794\n",
      "\n",
      "OVERLAP:\n",
      "  Users in both: 2034\n",
      "  Events in both: 1014\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = temporal_split_per_user(train_raw, train_ratio=0.5, min_interactions=3)\n",
    "\n",
    "print_split_stats(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processed Events (for geo coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached processed events...\n",
      "Processed events shape: (3137972, 113)\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import EventFeatureExtractor\n",
    "\n",
    "processed_events_path = Path(\"../data/processed/events_processed.csv\")\n",
    "\n",
    "if processed_events_path.exists():\n",
    "    print(\"Loading cached processed events...\")\n",
    "    events = pd.read_csv(processed_events_path)\n",
    "else:\n",
    "    print(\"Processing events...\")\n",
    "    extractor = EventFeatureExtractor(n_clusters=30)\n",
    "    events = extractor.fit_transform(events_raw)\n",
    "    events.to_csv(processed_events_path, index=False)\n",
    "\n",
    "print(f\"Processed events shape: {events.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Baseline with Geographic Filtering\n",
    "\n",
    "This baseline:\n",
    "1. Finds user's median location from past events\n",
    "2. Filters to top-K nearest events (geo_top_k)\n",
    "3. **Randomly samples 200 events** from this pool\n",
    "4. No learning - just geographic proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline function ready\n"
     ]
    }
   ],
   "source": [
    "def get_user_median_location(user_id, train_df, events):\n",
    "    \"\"\"Get user's median location from their past interactions\"\"\"\n",
    "    user_events = train_df[train_df[\"user\"] == user_id][\"event\"].tolist()\n",
    "    \n",
    "    if not user_events:\n",
    "        return None, None\n",
    "    \n",
    "    # Use merge instead of isin for better performance\n",
    "    event_locs = events[events[\"event_id\"].isin(user_events)][[\"lat\", \"lng\"]].dropna()\n",
    "    \n",
    "    if len(event_locs) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    median_lat = event_locs[\"lat\"].median()\n",
    "    median_lng = event_locs[\"lng\"].median()\n",
    "    \n",
    "    return median_lat, median_lng\n",
    "\n",
    "\n",
    "def random_recommend_with_geo(user_id, train_df, events, geo_top_k=3000, n=200, exclude_seen=True):\n",
    "    \"\"\"Random recommendation with geographic filtering\"\"\"\n",
    "    \n",
    "    # Get user location\n",
    "    user_lat, user_lng = get_user_median_location(user_id, train_df, events)\n",
    "    \n",
    "    if user_lat is None or pd.isna(user_lat) or pd.isna(user_lng):\n",
    "        # Fallback: random sample from all events\n",
    "        candidates = events[\"event_id\"].sample(min(geo_top_k, len(events))).tolist()\n",
    "    else:\n",
    "        # Pre-filter events with valid coordinates\n",
    "        valid_events = events.dropna(subset=[\"lat\", \"lng\"]).copy()\n",
    "        \n",
    "        # Vectorized distance calculation\n",
    "        valid_events[\"distance\"] = haversine_distance(\n",
    "            user_lat, user_lng, \n",
    "            valid_events[\"lat\"].values, \n",
    "            valid_events[\"lng\"].values\n",
    "        )\n",
    "        \n",
    "        # Get top-K nearest events\n",
    "        nearest_events = valid_events.nsmallest(geo_top_k, \"distance\")\n",
    "        candidates = nearest_events[\"event_id\"].tolist()\n",
    "    \n",
    "    # Exclude seen events\n",
    "    if exclude_seen:\n",
    "        seen_events = set(train_df[train_df[\"user\"] == user_id][\"event\"])\n",
    "        candidates = [e for e in candidates if e not in seen_events]\n",
    "    \n",
    "    # Randomly sample n events\n",
    "    if len(candidates) <= n:\n",
    "        return candidates\n",
    "    else:\n",
    "        return list(np.random.choice(candidates, size=n, replace=False))\n",
    "\n",
    "\n",
    "print(\"Random baseline function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with labels in validation: 1501\n",
      "Evaluating on 100 users...\n",
      "\n",
      "==================================================\n",
      "RANDOM BASELINE (geo_top_k=3000) @ K=200\n",
      "==================================================\n",
      "Recall@K            : 0.03833\n",
      "Hit_Rate@K          : 0.06000\n",
      "Contamination@K     : 0.00000\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "K = 200\n",
    "N_TEST_USERS = 100\n",
    "GEO_TOP_K = 3000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "val_with_labels = val_df[(val_df[\"interested\"] == 1) | (val_df[\"not_interested\"] == 1)]\n",
    "users_with_labels = val_with_labels[\"user\"].unique()\n",
    "\n",
    "print(f\"Users with labels in validation: {len(users_with_labels)}\")\n",
    "\n",
    "if N_TEST_USERS:\n",
    "    test_users = users_with_labels[:N_TEST_USERS]\n",
    "else:\n",
    "    test_users = users_with_labels\n",
    "\n",
    "print(f\"Evaluating on {len(test_users)} users...\")\n",
    "\n",
    "random_predictions = {}\n",
    "actuals = {}\n",
    "not_interested = {}\n",
    "\n",
    "for user in test_users:\n",
    "    random_predictions[user] = random_recommend_with_geo(\n",
    "        user, train_df, events, geo_top_k=GEO_TOP_K, n=K, exclude_seen=True\n",
    "    )\n",
    "    actuals[user] = val_df[(val_df[\"user\"] == user) & (val_df[\"interested\"] == 1)][\"event\"].tolist()\n",
    "    not_interested[user] = val_df[(val_df[\"user\"] == user) & (val_df[\"not_interested\"] == 1)][\"event\"].tolist()\n",
    "\n",
    "metrics = evaluate_recommendations(actuals, random_predictions, not_interested, k=K)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RANDOM BASELINE (geo_top_k={GEO_TOP_K}) @ K={K}\")\n",
    "print(f\"{'='*50}\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.5f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Different geo_top_k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 500\n",
      "============================================================\n",
      "Recall@200: 0.02760\n",
      "Hit_Rate@200: 0.07000\n",
      "Time: 195.9s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 1000\n",
      "============================================================\n",
      "Recall@200: 0.02176\n",
      "Hit_Rate@200: 0.06000\n",
      "Time: 239.6s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 2000\n",
      "============================================================\n",
      "Recall@200: 0.05310\n",
      "Hit_Rate@200: 0.10000\n",
      "Time: 250.4s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 3000\n",
      "============================================================\n",
      "Recall@200: 0.03833\n",
      "Hit_Rate@200: 0.06000\n",
      "Time: 255.3s\n",
      "\n",
      "============================================================\n",
      "Testing Random with geo_top_k = 5000\n",
      "============================================================\n",
      "Recall@200: 0.00667\n",
      "Hit_Rate@200: 0.02000\n",
      "Time: 251.0s\n",
      "\n",
      "============================================================\n",
      "SUMMARY - Random Baseline with Different geo_top_k\n",
      "============================================================\n",
      "geo_top_k       Recall@K        Hit_Rate@K     \n",
      "---------------------------------------------\n",
      "500             0.02760         0.07000        \n",
      "1000            0.02176         0.06000        \n",
      "2000            0.05310         0.10000        \n",
      "3000            0.03833         0.06000        \n",
      "5000            0.00667         0.02000        \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "geo_values = [500, 1000, 2000, 3000, 5000]\n",
    "results = []\n",
    "\n",
    "for geo_k in geo_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Random with geo_top_k = {geo_k}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions = {}\n",
    "    for user in test_users:\n",
    "        predictions[user] = random_recommend_with_geo(\n",
    "            user, train_df, events, geo_top_k=geo_k, n=K, exclude_seen=True\n",
    "        )\n",
    "    \n",
    "    metrics = evaluate_recommendations(actuals, predictions, not_interested, k=K)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        \"geo_top_k\": geo_k,\n",
    "        \"recall\": metrics[\"Recall@K\"],\n",
    "        \"hit_rate\": metrics[\"Hit_Rate@K\"],\n",
    "        \"contamination\": metrics[\"Contamination@K\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Recall@{K}: {metrics['Recall@K']:.5f}\")\n",
    "    print(f\"Hit_Rate@{K}: {metrics['Hit_Rate@K']:.5f}\")\n",
    "    print(f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY - Random Baseline with Different geo_top_k\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'geo_top_k':<15} {'Recall@K':<15} {'Hit_Rate@K':<15}\")\n",
    "print(f\"{'-'*45}\")\n",
    "for r in results:\n",
    "    print(f\"{r['geo_top_k']:<15} {r['recall']:<15.5f} {r['hit_rate']:<15.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Real Models\n",
    "\n",
    "Compare Random baseline with the actual models to see the improvement from learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON: Random Baseline vs Real Models\n",
      "======================================================================\n",
      "Model                                    Recall@200      Hit_Rate@200   \n",
      "----------------------------------------------------------------------\n",
      "Random (geo_top_k=1000)                  0.02176         0.06000        \n",
      "Random (geo_top_k=3000)                  0.03833         0.06000        \n",
      "Content-Based (geo_top_k=1000)           0.10376         0.16000        \n",
      "Collaborative (geo_top_k=3000)           0.27231         0.34000        \n",
      "Social (no geo filter)                   0.46997         0.59184        \n",
      "\n",
      "======================================================================\n",
      "ANALYSIS: How much do models improve over random?\n",
      "======================================================================\n",
      "\n",
      "Content-Based (geo_top_k=1000):\n",
      "  Random baseline:  0.02176\n",
      "  Content-Based:    0.10376\n",
      "  Improvement:      4.8x\n",
      "\n",
      "Collaborative (geo_top_k=3000):\n",
      "  Random baseline:  0.03833\n",
      "  Collaborative:    0.27231\n",
      "  Improvement:      7.1x\n",
      "\n",
      "Social (no geo filter, compared to random@3000):\n",
      "  Random baseline:  0.03833\n",
      "  Social:           0.46997\n",
      "  Improvement:      12.3x\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION:\n",
      "======================================================================\n",
      "✓ All models significantly beat random baseline!\n",
      "  - Content-Based learns useful patterns (4.8x improvement)\n",
      "  - Collaborative learns strong patterns (7.1x improvement)\n",
      "  - Social learns exceptional patterns (12.3x improvement)\n",
      "\n",
      "This proves that models are NOT just benefiting from geographic filtering.\n",
      "They successfully learn meaningful user preferences and event characteristics.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Best results from each model (using optimal geo_top_k for each)\n",
    "comparison = [\n",
    "    {\"model\": \"Random (geo_top_k=1000)\", \"recall\": 0.02176, \"hit_rate\": 0.06000, \"geo_k\": 1000},\n",
    "    {\"model\": \"Random (geo_top_k=3000)\", \"recall\": 0.03833, \"hit_rate\": 0.06000, \"geo_k\": 3000},\n",
    "    {\"model\": \"Content-Based (geo_top_k=1000)\", \"recall\": 0.10376, \"hit_rate\": 0.16000, \"geo_k\": 1000},\n",
    "    {\"model\": \"Collaborative (geo_top_k=3000)\", \"recall\": 0.27231, \"hit_rate\": 0.34000, \"geo_k\": 3000},\n",
    "    {\"model\": \"Social (no geo filter)\", \"recall\": 0.46997, \"hit_rate\": 0.59184, \"geo_k\": None},\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON: Random Baseline vs Real Models\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Model':<40} {'Recall@200':<15} {'Hit_Rate@200':<15}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for item in comparison:\n",
    "    print(f\"{item['model']:<40} {item['recall']:<15.5f} {item['hit_rate']:<15.5f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANALYSIS: How much do models improve over random?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Content-Based vs Random @ geo_top_k=1000\n",
    "random_cb = comparison[0][\"recall\"]\n",
    "cb_recall = comparison[2][\"recall\"]\n",
    "cb_improvement = (cb_recall / random_cb)\n",
    "\n",
    "print(f\"\\nContent-Based (geo_top_k=1000):\")\n",
    "print(f\"  Random baseline:  {random_cb:.5f}\")\n",
    "print(f\"  Content-Based:    {cb_recall:.5f}\")\n",
    "print(f\"  Improvement:      {cb_improvement:.1f}x\")\n",
    "\n",
    "# Collaborative vs Random @ geo_top_k=3000\n",
    "random_cf = comparison[1][\"recall\"]\n",
    "cf_recall = comparison[3][\"recall\"]\n",
    "cf_improvement = (cf_recall / random_cf)\n",
    "\n",
    "print(f\"\\nCollaborative (geo_top_k=3000):\")\n",
    "print(f\"  Random baseline:  {random_cf:.5f}\")\n",
    "print(f\"  Collaborative:    {cf_recall:.5f}\")\n",
    "print(f\"  Improvement:      {cf_improvement:.1f}x\")\n",
    "\n",
    "# Social vs Random @ geo_top_k=3000 (for comparison)\n",
    "social_recall = comparison[4][\"recall\"]\n",
    "social_improvement = (social_recall / random_cf)\n",
    "\n",
    "print(f\"\\nSocial (no geo filter, compared to random@3000):\")\n",
    "print(f\"  Random baseline:  {random_cf:.5f}\")\n",
    "print(f\"  Social:           {social_recall:.5f}\")\n",
    "print(f\"  Improvement:      {social_improvement:.1f}x\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSION:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"✓ All models significantly beat random baseline!\")\n",
    "print(f\"  - Content-Based learns useful patterns ({cb_improvement:.1f}x improvement)\")\n",
    "print(f\"  - Collaborative learns strong patterns ({cf_improvement:.1f}x improvement)\")\n",
    "print(f\"  - Social learns exceptional patterns ({social_improvement:.1f}x improvement)\")\n",
    "print()\n",
    "print(\"This proves that models are NOT just benefiting from geographic filtering.\")\n",
    "print(\"They successfully learn meaningful user preferences and event characteristics.\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event-recsys-mvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
